{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Conversations","text":"<p>Introducing \"Conversations\" - a Python package designed to explore the true potential of conversational analysis. With its ability to transcribe, diarise, and generate visually appealing HTML reports, \"Conversation\" empowers you to delve deeper into the intricacies of human communication. Recognizing that conversations are brimming with meaning conveyed not just through language, but also through key acoustic features like loudness, intonation, and speech rate, this package brings forth an indispensable tool for understanding and interpreting the wealth of information embedded in everyday exchanges. <code>Conversations</code> enables you to analyse and comprehend the complex world of human interaction.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Comprehensive documentation, including overview, API, and examples, can be found here.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install conversations\n</code></pre> <p>Install the optional dependencies for local processing with:</p> <pre><code>pip install conversations[local]\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<pre><code>from pathlib import Path\nfrom conversations import Conversation\n\n# Information about the conversation\naudio_file = Path('/path/to/audio.mp4')\nspeaker_mapping={\"0\": \"Alice\", \"1\": \"Bob\", \"2\": \"Sam\"}\n\n# Load the conversation\nconversation = Conversation(recording=audio_file, speaker_mapping=speaker_mapping)\n\n# Process the conversation\nconversation.transcribe(custom_terms=[\"Alice\", \"Bob\"])\nconversation.diarise()\nconversation.save()\n\n# Generate an interactive HTML version of the conversation\nhtml_report = conversation.report()\nwith open('conversation.html', 'w') as f:\n    f.write(html_report.render())\n\n# Generate a text file of the conversation\ntext_report = conversation.export_text()\nwith open('conversation.txt', 'w') as f:\n    f.write(text_report)\n\n# Generate a text file summarising the conversation\nsummary = conversation.summarise()\nwith open('conversation-summary.txt', 'w') as f:\n    f.write(summary)\n\n# Query the conversation\nanswer = conversation.query(\"What was the value of operating cash flow we were discussing?\")\n\n\nprint(f\"The answer to your query is: {answer}\")    \nprint(summary)\n\n\n# The answer to your query is: The value of operating cash \n# flow being discussed was $210 million in the last 12 months.\n\n\n# Summary:\n# In the meeting, Alice, Sam and Bob discuss the financial state of\n# businesses, the challenges of companies going private, and the\n# evolution of private equity firms' return expectations. They\n# also touch on the industry's early risk-taking nature. Bob\n# mentions historical deals as examples of the early risk-taking\n# nature of private equity firms.\n# \n# Key Points:\n# 1. Challenges of companies going private\n#    Alice: \"Challenges of companies going private, especially in\n#    terms of cutting expenses and finding new ways to pay\n#    employees who were previously compensated with stocks.\"\n# 2. Private equity firms' return expectations\n#    Sam: \"What are private equity firms' return expectations\n#    when purchasing a company?\"\n# 3. Evolution of the private equity industry\n#    Bob: \"The industry has evolved over time, with early private\n#    equity firms taking on riskier ventures,\n#    similar to venture capitalists.\"\n# ...\n# \n# Action Items:\n# None discussed in the meeting.\n# \n# 20 Keywords (most to least relevant):\n# 1. Financial state\n# 2. Business\n# 3. Operating cash flow\n# ...\n# \n# 10 Concepts/Themes (most to least relevant):\n# 1. Financial challenges\n# 2. Going private\n# 3. Private equity\n# ...\n# \n# Most unexpected aspect of the conversation:\n# The most unexpected aspect of the conversation was the mention of\n# historical deals like RJR Nabisco and TWA Airlines as examples\n# of the early risk-taking nature of private equity firms.\n# \n# Concepts/Topics explained in the transcript:\n# 1. Private equity firms' return expectations:\n#    Bob explains that the industry has evolved over time,\n#    with early private equity firms taking on riskier ventures,\n#    similar to venture capitalists. No inaccuracies were identified.\n# \n# Tone of the conversation:\n# Overall tone: Informative and analytical\n# Alice's tone: Concerned and analytical\n# Bob's tone: Informative and explanatory\n# Sam's tone: Inquisitive\n</code></pre>"},{"location":"api/","title":"Application Programming Interface","text":""},{"location":"api/#conversations-interface","title":"Conversations Interface","text":""},{"location":"api/#conversations.Conversation","title":"<code>conversations.Conversation</code>","text":"<p>Conversations class.</p> <p>This class is the core of the Conversations package. Use this class to manage your conversation and processing.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; conversation = Conversation(recording=Path(\"/path/to/file.m4a\"))\n&gt;&gt;&gt; conversation.transcribe()\n&gt;&gt;&gt; conversation.diarise()\n&gt;&gt;&gt; html_report = conversation.report()\n</code></pre>"},{"location":"api/#conversations.Conversation.__init__","title":"<code>__init__(recording, num_speakers=0, reload=True, speaker_mapping=None, meeting_datetime=None, attendees=None, transcription=None, transcription_shortened=None, summary=None, summary_automated=None)</code>","text":"<p>Initialise Conversations class.</p> <p>Parameters:</p> Name Type Description Default <code>recording</code> <code>Path</code> <p>Path to the conversation recording.</p> required <code>num_speakers</code> <code>int</code> <p>The number of speakers in the conversation, defaulting to 0 if unknown.</p> <code>0</code> <code>reload</code> <code>bool</code> <p>If True, try to load an existing saved conversation with the default filename. If False, create a new Conversation instance.</p> <code>True</code> <code>speaker_mapping</code> <code>Optional[Dict[str, str]]</code> <p>A dictionary mapping speaker IDs to speaker names.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>conversation</code> <code>Conversation</code> <p>Instance of Conversation.</p>"},{"location":"api/#conversations.Conversation.transcribe","title":"<code>transcribe(method='assembly', model='nano', prompt=\" - How are you? - I'm fine, thank you.\", language='en', custom_terms=None)</code>","text":"<p>Transcribe a conversation using the specified method and model.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The transcription method to use, defaults to \"assembly\". Can be one of whisper or assembly.</p> <code>'assembly'</code> <code>model</code> <code>str</code> <p>The model to be used for transcription, must be one of tiny, base, small, large, medium, tiny.en, base.en, small.en, medium.en, and openai.en, best, slam-1, universal, nano. Defaults to \"nano\".</p> <code>'nano'</code> <code>prompt</code> <code>(str, None)</code> <p>An optional prompt to be used for transcription, defaults to None.</p> <code>\" - How are you? - I'm fine, thank you.\"</code> <code>language</code> <code>str</code> <p>The language of the conversation, defaults to \"en\".</p> <code>'en'</code> <code>custom_terms</code> <code>list[str]</code> <p>Custom terms passed to the transcription engine. Currently only supported when using the AssemblyAI <code>\"slam-1\"</code> model.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the conversation has already been transcribed.</p> <code>ValueError</code> <p>If an invalid model name is provided.</p> <code>NotImplementedError</code> <p>If the method is not supported.</p>"},{"location":"api/#conversations.Conversation.diarise","title":"<code>diarise(method='simple')</code>","text":"<p>Diarise a conversation.</p>"},{"location":"api/#conversations.Conversation.shortened_transcript","title":"<code>shortened_transcript(chunk_num_tokens=128000, shorten_iterations=2)</code>","text":"<p>Get the shortened transcript of the conversation.</p> <p>This method returns the previously shortened transcript if available. If not, it computes the shortened transcript first and then returns it.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_num_tokens</code> <code>int</code> <p>The number of tokens to use for each chunk. The default is 7372, which is 90% of the 8k model limit.</p> <code>128000</code> <code>shorten_iterations</code> <code>int</code> <p>The number of iterations to use when shortening the transcript.</p> <code>2</code> <p>Returns:</p> Type Description <code>str</code> <p>The shortened transcript of the conversation.</p>"},{"location":"api/#conversations.Conversation.summarise","title":"<code>summarise(force=False, print_summary=True, system_prompt=None, summary_prompt=None, append_prompt=None)</code>","text":"<p>Generate a summary of the conversation.</p> <p>Parameters:</p> Name Type Description Default <code>force</code> <code>bool</code> <p>If True, generate a new summary even if one already exists.</p> <code>False</code> <code>print_summary</code> <code>bool</code> <p>If True, print the summary to the console.</p> <code>True</code> <code>system_prompt</code> <code>str or None</code> <p>The system prompt to use when generating the summary.</p> <code>None</code> <code>summary_prompt</code> <code>str or None</code> <p>The summary prompt to use when generating the summary.</p> <code>None</code> <code>append_prompt</code> <code>str or None</code> <p>The append prompt to use when generating the summary.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>summary</code> <code>str</code> <p>The generated summary.</p>"},{"location":"api/#conversations.Conversation.query","title":"<code>query(query, print_summary=True, system_prompt=None, append_prompt=None)</code>","text":"<p>Query the conversation.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to ask the conversation.</p> required <code>print_summary</code> <code>bool</code> <p>If True, print the summary to the console.</p> <code>True</code> <code>system_prompt</code> <code>str or None</code> <p>The system prompt to use when generating the summary.</p> <code>None</code> <code>append_prompt</code> <code>str or None</code> <p>The append prompt to use when generating the summary.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>summary</code> <code>str</code> <p>The generated query response.</p>"},{"location":"api/#conversations.Conversation.save","title":"<code>save(file_path=None)</code>","text":"<p>Save the Conversation object to disk.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Optional[str]</code> <p>The path where the Conversation object will be saved. If None, a default filename based on the audio file path will be used.</p> <code>None</code>"},{"location":"api/#conversations.Conversation.report","title":"<code>report(audio_file=None)</code>","text":"<p>Generate a report of a conversation.</p>"},{"location":"api/#conversations.Conversation.export_text","title":"<code>export_text()</code>","text":"<p>Export the transcript and diarisation as text.</p>"},{"location":"api/#conversations.load_conversation","title":"<code>conversations.load_conversation(file_path)</code>","text":"<p>Load a Conversation object from disk.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the file containing the saved Conversation object.</p> required <p>Returns:</p> Name Type Description <code>conversation</code> <code>Conversation</code> <p>The loaded Conversation object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the loaded object is not an instance of the Conversation class.</p>"},{"location":"api/#low-level-interface","title":"Low-Level Interface","text":""},{"location":"api/#transcription","title":"Transcription","text":""},{"location":"api/#conversations.transcribe.whisper.process","title":"<code>conversations.transcribe.whisper.process(audio_file, model_name='base.en', prompt=None, language='en')</code>","text":"<p>Transcribe audio using Whisper.</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>Path</code> <p>Path to the audio file.</p> required <code>model_name</code> <code>str</code> <p>Name of the whisper model to use. To use the cloud service provided by OpenAI, use \"openai.en\", by default \"base.en\".</p> <code>'base.en'</code> <code>prompt</code> <code>str</code> <p>Prompt to use for the transcription, by default None.</p> <code>None</code> <code>language</code> <code>str</code> <p>Language to use for the transcription, by default \"en\".</p> <code>'en'</code> <p>Returns:</p> Name Type Description <code>transcript</code> <code>Dict[str, str]</code> <p>Dictionary containing the audio transcript in whisper format.</p>"},{"location":"api/#diarisation","title":"Diarisation","text":""},{"location":"api/#conversations.diarise.simple.process","title":"<code>conversations.diarise.simple.process(audio_file, num_speakers=2)</code>","text":"<p>Diarise audio using simple_diarizer.</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>Path</code> <p>Path to the audio file.</p> required <code>num_speakers</code> <code>int</code> <p>The number of speakers in the conversation.</p> <code>2</code> <p>Returns:</p> Name Type Description <code>segments</code> <code>list[dict]</code> <p>List containing the segments as dictionaries.</p>"},{"location":"api/#report","title":"Report","text":""},{"location":"api/#conversations.report.generate","title":"<code>conversations.report.generate(transcript, diarisation=None, audio_file=None, speaker_mapping=None)</code>","text":"<p>Create html page from conversation.</p> <p>Parameters:</p> Name Type Description Default <code>transcript</code> <code>dict</code> <p>Transcript from transcribe module.</p> required <code>diarisation</code> <code>dict</code> <p>Speaker diarisation from from diarisation module.</p> <code>None</code> <code>audio_file</code> <code>PosixPath | None</code> <p>Path to audio file. If provided, audio will be embedded in the report and timestamps will link to audio times.</p> <code>None</code> <code>speaker_mapping</code> <code>dict</code> <p>Mapping of old speaker names to new speaker names. For example, {\"0\": \"Alice\", \"1\": \"Bob\"}.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>report</code> <code>document</code> <p>Conversation report as a dominate document.</p>"},{"location":"api/#conversations.report.export_text","title":"<code>conversations.report.export_text(transcript, diarisation=None, speaker_mapping=None, datetimestr=None, attendees=None)</code>","text":"<p>Create text file from conversation.</p> <p>Exports the transcript as a text document while grouping segments by speaker (if diarisation is provided) and mapping speaker names (if speaker_mapping is provided).</p> <p>Parameters:</p> Name Type Description Default <code>transcript</code> <code>Dict[str, Any]</code> <p>The transcript data structured as a dictionary with a key \"segments\" containing a list of dicts. Each dict has a \"text\" key with the corresponding transcript segment text and optionally a \"speaker\" key.</p> required <code>diarisation</code> <code>Optional[List[Dict[str, Any]]]</code> <p>A list of diarisation data dictionaries. Each dictionary should have the keys \"start\", \"end\", and \"speaker\".</p> <code>None</code> <code>speaker_mapping</code> <code>Optional[Dict[str, str]]</code> <p>A dictionary that maps speaker identifiers to speaker names. The key is the speaker identifier and the value is the speaker name.</p> <code>None</code> <code>datetimestr</code> <code>Optional[str]</code> <p>A string representing the date and time of the meeting.</p> <code>None</code> <code>attendees</code> <code>Optional[List[str]]</code> <p>A list of attendees in the meeting.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The transcript exported as a text document.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; transcript = {\"segments\": [{\"text\": \"Hello, world!\", \"speaker\": \"speaker_1\"}]}\n&gt;&gt;&gt; diarisation = None\n&gt;&gt;&gt; speaker_mapping = {\"speaker_1\": \"John\"}\n&gt;&gt;&gt; export_text(transcript, diarisation, speaker_mapping)\n'John: Hello, world!'\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v0110-2022-12-18","title":"v0.11.0 (2022-12-18)","text":""},{"location":"changelog/#feature","title":"Feature","text":"<ul> <li>Generate reports (<code>fd5ea4c</code>)</li> </ul>"},{"location":"changelog/#v0100-2022-12-18","title":"v0.10.0 (2022-12-18)","text":""},{"location":"changelog/#feature_1","title":"Feature","text":"<ul> <li>Add transcription using whisper (<code>a9ede7c</code>)</li> </ul>"},{"location":"changelog/#documentation","title":"Documentation","text":"<ul> <li>Use mkdocs for documentation (<code>d2b3b0a</code>)</li> </ul>"},{"location":"changelog/#v090-2022-11-25","title":"v0.9.0 (2022-11-25)","text":""},{"location":"changelog/#feature_2","title":"Feature","text":"<ul> <li>Improve commit title for releases (<code>78965c3</code>)</li> </ul>"},{"location":"changelog/#v080-2022-11-25","title":"v0.8.0 (2022-11-25)","text":""},{"location":"changelog/#feature_3","title":"Feature","text":"<ul> <li>What happens if we have a longer message (<code>77a2cce</code>)</li> </ul>"},{"location":"changelog/#v070-2022-11-25","title":"v0.7.0 (2022-11-25)","text":""},{"location":"changelog/#feature_4","title":"Feature","text":"<ul> <li>Update committer name (<code>23a2318</code>)</li> </ul>"},{"location":"changelog/#v060-2022-11-25","title":"v0.6.0 (2022-11-25)","text":""},{"location":"changelog/#feature_5","title":"Feature","text":"<ul> <li>Commit changes to github (<code>a9d06d5</code>)</li> </ul>"},{"location":"contributing/","title":"Contributing to Conversations","text":"<p>Contributions are immensely appreciated and welcomed.</p>"},{"location":"contributing/#development-process","title":"Development Process","text":""},{"location":"contributing/#standards","title":"Standards","text":"<p>To ensure effecient collaborative development, a variety of standards are utilised in this project.</p> <ul> <li>Semantic Versioning is used.</li> <li>Python Semantic Releases      is used to automate change log generation and releases.</li> <li>Conventional Commits are utilised    and validated using the wagoid/commitlint-github-action    github action, which itself uses commitlint</li> <li>Conventional Commmits are also utilised for     pull request titles and enforced using     amannn/action-semantic-pull-request.</li> <li>Black code formatter is used.</li> <li>Actions Black      is used to format code in PRs.</li> <li>Numpy style documentation strings    are used.</li> <li>Pydocstyle is used to ensure documentation      strings adhere to the standard.</li> <li>Type hinting is used.</li> <li>And checked using mypy.</li> <li>Spelling is enforced using codespell.</li> </ul>"},{"location":"contributing/#website","title":"Website","text":"<ul> <li>The website and documentation is generated by mkdocs   with the mkdocsstrings   extension and material   theme.</li> </ul>"},{"location":"contributing/#running-tests-locally","title":"Running Tests Locally","text":"<ul> <li>Ensure you have hatch installed:</li> <li><code>pip install --upgrade hatch</code></li> <li>To run tests locally, execute:</li> <li><code>hatch run cov</code></li> <li>If you have modified the environment</li> <li>List the environments using <code>hatch env show --ascii</code></li> <li>To remove all environments and start fresh <code>hatch env prune</code></li> </ul>"},{"location":"contributing/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>On a Mac, you may need to brew install xv and ffmpeg.</li> </ul>"},{"location":"examples/","title":"Example Analysis","text":""},{"location":"examples/#example-1-three-speaker-podcast","title":"Example 1 - Three Speaker Podcast","text":"<p>In the following example we analyse a podcast with three speakers. The audio file is pulled from an S3 bucket and the speaker mapping is provided. We ask the <code>Conversations</code> package to transcribe, diarise, summarise and generate an interactive HTML version of the transcription. To view the html version of the transcript click on the blue button below.</p> podcast_example.py<pre><code># Description: Example of using the Conversation class to analyse a podcast\nfrom pathlib import Path\nfrom conversations import Conversation\nimport pooch\n\n# Properties of the conversation\ncloud_file = \"https://project-test-data-public.s3.amazonaws.com/test_audio.m4a\"\nspeaker_mapping = {\"0\": \"Alice\", \"1\": \"Bob\", \"2\": \"Sam\"}\n\n# Download audio file\naudio_file = pooch.retrieve(\n    url=cloud_file,\n    known_hash=\"md5:77d8b60c54dffbb74d48c4a65cd59591\",\n    fname=\"podcast.m4a\"\n)\n\n# Process the conversation\nconversation = Conversation(recording=Path(audio_file),\n                            num_speakers=3,\n                            speaker_mapping=speaker_mapping)\nconversation.transcribe(model=\"openai.en\")\nconversation.diarise()\nconversation.save()\n\n# Generate an interactive HTML version of the conversation\nhtml_report = conversation.report(audio_file=cloud_file)\nwith open('conversation.html', 'w') as f:\n    f.write(html_report.render())\n\n# Generate a text file of the conversation\ntext_report = conversation.export_text()\nwith open('conversation.txt', 'w') as f:\n    f.write(text_report)\n\n# Generate a text file summarising the conversation\nsummary = conversation.summarise()\nprint(summary)\n</code></pre> <p>Interactive HTML Report</p> <pre><code># Summary:\n# In the meeting, Alice, Sam and Bob discuss the financial state of\n# businesses, the challenges of companies going private, and the\n# evolution of private equity firms' return expectations. They\n# also touch on the industry's early risk-taking nature. Bob\n# mentions historical deals as examples of the early risk-taking\n# nature of private equity firms.\n# \n# Key Points:\n# 1. Challenges of companies going private\n#    Alice: \"Challenges of companies going private, especially in\n#    terms of cutting expenses and finding new ways to pay\n#    employees who were previously compensated with stocks.\"\n# 2. Private equity firms' return expectations\n#    Sam: \"What are private equity firms' return expectations\n#    when purchasing a company?\"\n# 3. Evolution of the private equity industry\n#    Bob: \"The industry has evolved over time, with early private\n#    equity firms taking on riskier ventures,\n#    similar to venture capitalists.\"\n# ...\n# \n# Action Items:\n# None discussed in the meeting.\n# \n# 20 Keywords (most to least relevant):\n# 1. Financial state\n# 2. Business\n# 3. Operating cash flow\n# ...\n# \n# 10 Concepts/Themes (most to least relevant):\n# 1. Financial challenges\n# 2. Going private\n# 3. Private equity\n# ...\n# \n# Most unexpected aspect of the conversation:\n# The most unexpected aspect of the conversation was the mention of\n# historical deals like RJR Nabisco and TWA Airlines as examples\n# of the early risk-taking nature of private equity firms.\n# \n# Concepts/Topics explained in the transcript:\n# 1. Private equity firms' return expectations:\n#    Bob explains that the industry has evolved over time,\n#    with early private equity firms taking on riskier ventures,\n#    similar to venture capitalists. No inaccuracies were identified.\n# \n# Tone of the conversation:\n# Overall tone: Informative and analytical\n# Alice's tone: Concerned and analytical\n# Bob's tone: Informative and explanatory\n# Sam's tone: Inquisitive\n</code></pre>"}]}